# HDBSCANの仕組み

HorseMLの開発をしていて、リリースのお知らせをJuliaのDiscouseに書き込んだところ、HDBSCANやDBSCANのサポートを行う予定はありますか？との質問を受けたので、調べてみるとどうやらデータ密度からクラスを特定する手法のよう。つまり、K-meansのようにクラスタ数を最初に与えなくて良いのですが、この他にも、K-meansの拡張版であるX-meansでもBIC（ベイズ情報量）を用いてクラスタ数を決定する方法もあります。
DBSCANの実装はそう難しくなかったのですが、HDBSCANの理解・実装に時間がかかったので、備忘録として残します。

## HDBSCANのアルゴリズム
ほぼ、参考文献に載せた論文の2.2.2の文章を和訳したものです。

### 中心距離の定義
このアルゴリズムには、対象となるデータの他に、２つのパラメータが必要となります。
１つ目のパラメータは距離の算出に使用する最小のポイントの数`k`です。もう一つは、クラスタの形成に必要な最小のクラスタサイズ`min_clusters`です。
最初のステップはデータ同士の距離を求めることです。まず、それぞれの点について中心距離を定義します。中心距離は、「ある点からk番目に近い点からの距離」として定義されます。

この中心距離を用いて、相互到達可能距離(mutual reachability distance)と呼ばれる新しい測定基準が計算できます。相互到達可能距離は、2点それぞれの中心距離と2点間の距離（ユークリッド距離）の最大値です。

### 最小全域木
次に、相互到達可能距離はデータ点を頂点として持ち、どの2点の間のエッジも相互到達可能距離の重みを持つグラフを作成するのに使用できます。これは計算する必要のあるグラフの最小全域木なので、アルゴリズムの人工的な概念に過ぎません。最小全域木とは全域木のエッジの重みの合計が最小になるものです。
つまり、最小全域木は閉路を持たず、エッジの重みの合計が最小となる完全なグラフに含まれるエッジによってすべてのノードが連結されたものです。結果の最小全域木はそれぞれの相互到達可能距離を重みとするループ（self-edge,loopとも呼ばれます）を各頂点に加えることで変更されます。これにより拡張最小全域木と呼ばれるものが得られます。

これで、グラフを使用してHDBSCAN階層を構築できます。 まず、クラスターラベルが1つあり、すべてのポイントがこのクラスターに割り当てられます。 このクラスターはクラスターリストに追加されます。 次に、グラフはエッジの重みで昇順で並べ替えられます。グラフの下から順に、拡張された最小スパニングツリーからエッジが繰り返し削除されます。同じ重みのエッジも同時に削除する必要があります。削除されるエッジの重み値は、現在の階層レベルを示すために使用されます。エッジが削除されると、削除されたエッジを含むクラスターが探索されます。切断され、 `min_size`より少ないポイントを含むクラスターには、すべてノイズラベルが割り当てられます。 切断されたが`min_size`ポイントを超えるクラスターには、新しいクラスターラベルが割り当てられます。 これはクラスター分割と呼ばれます。更に、新しいクラスタはクラスタのリストに追加されます。エッジの削除によるクラスタ分割が原因で新たなクラスタが生成された時に、新たな階層レベルが作られます。最終的には、データセットに含まれるすべてのデータがノイズに割り振られます。このような過程によって作られた階層がHDBSCAN階層です。HDBSCAN階層を作成する間、すべてのクラスタは保持され、それぞれの親クラスタへの参照を持っています。

### クラスタの識別
次のステップでは、HDBSCAM階層と作成されたクラスタのリストを使って特徴的なクラスタを階層の中から識別することです。これを行うには、クラスタの安定性を決定するのに使うことのできる新たな基準が確立される必要があります。これはラムダと呼ばれ、$\lambda = \frac{1}{edgeweight}$というものです。更に、クラスタにも$\lambda_{birth}$と$\lambda_{death}$をそれぞれクラスタが生成された時のラムダ、そのクラスタが2つのクラスタに分割された際のラムダとして定義します。クラスタ内のそれぞれの点には、その点がクラスタから落ちた時のラムダを$\lambda_{p}$として定義できます。これで安定性をクラスタに対して計算することができます。このように表現できます。
```math
stability = \sum_{p \in Cluster}(\lambda_{p} - \lambda_{birth})
```

この安定性はクラスタを通して伝播される必要があります。末端のクラスタは子クラスタを持たず、クラスタのリストから識別することができます。末端のクラスタから始めてクラスタの持つ参照を用いて上の階層のクラスタへと移って行きます。末端のクラスタは常にその安定性を親クラスタに伝播するともに伝搬された子クラスたとして親クラスタに追加します。末端でないクラスタには2つのうちどちらかが起こります。もし処理中のクラスタの安定性が子クラスタの合計安定性より高い場合、その安定性だけが親クラスタへ伝えられます。その他の場合、その子クラスタの安定性の合計が親クラスタに伝えられます。自明ですが、ルートクラスタでは親クラスタへの伝播は発生しません。
処理が終わると、ルートクラスタには安定性が最も高い子クラスタへの参照が含まれています。最も高い安定性を持つクラスタは最も特徴的なクラスタです。最も特徴的なクラスタの詳細とともにHDBSCAN階層を使用すると、各データポイントのクラスタ割当が高速に生成できます。

## 最後に
大分複雑なので、「ん？」となるところも結構ありますが、なんとか理解できるかなって感じです。
難しいアルゴリズムではありますが、他の手法とは段違いにノイズに強いので役に立つものだと思います（けどHorseMLに実装するのはかなり骨が折れそう…）。

## 参考文献
- [An Implementation of the HDBSCAN* Clustering Algorithm](https://www.mdpi.com/2076-3417/12/5/2405)
- [How HDBSCAN Works](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html)